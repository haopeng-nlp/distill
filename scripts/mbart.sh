root=/gscratch/ark/ivyg/fasttext-debias
source $root/scripts/config.sh

src=en
tgt=ro
lang_pair=ro-en

SCRIPTS=$root/mosesdecoder/scripts
TOKENIZER=$SCRIPTS/tokenizer/tokenizer.perl
LC=$SCRIPTS/tokenizer/lowercase.perl
BPEROOT=$root/distill/fairseq/subword-nmt/subword_nmt
BPE_TOKENS=20000
BPE_CODE=$root/models/mbart.cc25.v2/$lang_pair.code

dest=$root/distill/mbart/data/$lang_pair
train=$dest/train.$lang_pair

### train - copy

# cp $root/distill/mbart/train/europarl-v8.$lang_pair.en $dest/train.$src
# cp $root/distill/mbart/train/europarl-v8.$lang_pair.ro $dest/train.$tgt

### valid/test - sgml to plain text

# python $root/scripts/strip_sgml.py \
#     --input $root/distill/mbart/dev/newsdev2016-enro-src.en.sgm \
#     --output $dest/valid.$src
# python $root/scripts/strip_sgml.py \
#     --input $root/distill/mbart/dev/newsdev2016-enro-ref.ro.sgm \
#     --output $dest/valid.$tgt

### preprocessing

# echo "preprocess text"
# for split in train valid; do
#     for l in $src $tgt; do
#         echo $split.$l
#         cat $dest/$split.$l | $LC \
#         | $REPLACE_UNICODE_PUNCT \
#         | $NORM_PUNC -l $l \
#         | $REM_NON_PRINT_CHAR \
#         | $NORMALIZE_ROMANIAN \
#         | $REMOVE_DIACRITICS \
#         | $TOKENIZER -no-escape -l $l \
#         > $dest/$split.$l.clean
#     done
# done

### learn and apply bpe

# rm $train
# for l in $src $tgt; do
#     cat $dest/train.$l.clean >> $train
# done

# echo "learn_bpe.py on $train..."
# python $BPEROOT/learn_bpe.py -s $BPE_TOKENS < $train > $BPE_CODE

# for split in train valid; do
#     for l in $src $tgt; do
#         echo "apply_bpe.py to $split.$l..."
#         python $BPEROOT/apply_bpe.py -c $BPE_CODE < $dest/$split.$l > $dest/bpe.$split.$l
#     done
# done

### binarize data

# rm $root/distill/mbart/data-bin/$lang_pair/dict*
# rm $root/distill/mbart/data-bin/$lang_pair/preprocess.log

fairseq-preprocess \
    --srcdict $root/models/mbart.cc25.v2/dict.txt \
    --tgtdict $root/models/mbart.cc25.v2/dict.txt \
    --source-lang $src \
    --target-lang $tgt \
    --trainpref $dest/bpe.train \
    --validpref $dest/bpe.valid \
    --destdir $root/distill/mbart/data-bin/$lang_pair \
    --workers 20

### binary to plain text (generated by teacher with dropout)

noise=tdrop0.0
echo $noise

mkdir -p $DATA
mkdir -p $DATA_BIN

## --retain-dropout AND --seed ###
# for split in valid; do
#     echo "fairseq-generate ${split}"
#     dest=$DATA/$split

#     # removed:
#     #     --bpe 'sentencepiece' --sentencepiece-model $model_dir/sentence.bpe.model \
#     #     --sacrebleu --remove-bpe 'sentencepiece' \
#     nohup \
#     fairseq-generate $root/distill/mbart/data-bin/$lang_pair \
#         --path $model_dir/model.pt \
#         --task translation_from_pretrained_bart \
#         --gen-subset $split \
#         --max-tokens 1000 \
#         -s en -t ro \
#         --batch-size 128 --langs $langs \
#     > $dest.out
    
#     # https://github.com/facebookresearch/fairseq/issues/1758
#     # src_lang=${src:0:2}
#     # cat $dest.out | grep -P "^S" | cut -f 2- | sed 's/\['$src'\]//g' > $dest.src.temp
#     # $spm_decode --model=${bpe_model} --input=$dest.src.temp \
#     # | $REPLACE_UNICODE_PUNCT \
#     # | $NORM_PUNC -l $src_lang \
#     # | $REM_NON_PRINT_CHAR \
#     # | $NORMALIZE_ROMANIAN \
#     # | $REMOVE_DIACRITICS \
#     # | $TOKENIZER -no-escape -l $src_lang \
#     # > $dest.src
    
#     tgt_lang=${tgt:0:2}
#     cat $dest.out | grep -P "^H" | cut -f 3- | sed 's/\['$tgt'\]//g' > $dest.sys.temp
#     cat $dest.out | grep -P "^T" | cut -f 2- | sed 's/\['$tgt'\]//g' > $dest.ref.temp
#     for file in $dest.sys $dest.ref; do
#         $spm_decode --model=${bpe_model} --input=$file.temp \
#         | $REPLACE_UNICODE_PUNCT \
#         | $NORM_PUNC -l $tgt_lang \
#         | $REM_NON_PRINT_CHAR \
#         | $NORMALIZE_ROMANIAN \
#         | $REMOVE_DIACRITICS \
#         | $TOKENIZER -no-escape -l $tgt_lang \
#         > $file
#     done
#     sacrebleu -tok 'none' -s 'none' $dest.ref < $dest.sys
#     rm $DATA/*.temp
# done

## plain text to spm

# python ${spm_encode} --model=${bpe_model} < $DATA/train.src > $DATA/train.spm.$src
# python ${spm_encode} --model=${bpe_model} < $DATA/train.sys > $DATA/train.spm.$tgt

### spm to binary (train)

# fairseq-preprocess \
#     --source-lang $src \
#     --target-lang $tgt \
#     --srcdict $DICT \
#     --tgtdict $DICT \
#     --trainpref $DATA/train.spm \
#     --destdir $DATA_BIN \
#     --thresholdtgt 0 \
#     --thresholdsrc 0 \
#     --workers 70

##

# DIR=$root/distill/mbart/data-bin/$lang_pair
# echo "copying $(find $DIR/valid.* -printf "%f ")to $DATA_BIN"
# cp $DIR/valid.* $DATA_BIN
# # echo "copying $(find $DIR/test.* -printf "%f ")to $DATA_BIN"
# # cp $DIR/test.* $DATA_BIN
# # cmp --silent $old $new || echo "files are different"
