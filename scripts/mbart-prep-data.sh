export CUDA_VISIBLE_DEVICES=0,1,2,3
root=/net/nfs.cirrascale/allennlp/haop/fasttext-debias
langs=ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_lT,lv_lV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_lK,tr_TR,vi_VN,zh_CN

lang_pair=$1

if [[ $lang_pair == en-tr ]]; then
    src=en_XX
    tgt=tr_TR
    model_dir=$root/models/mbart-entr
    gold_dir=$root/distill/mbart/wmt_en_tr
elif [[ $lang_pair == tr-en ]]; then
    src=tr_TR
    tgt=en_XX
    model_dir=$root/models/mbart-tren
    gold_dir=$root/distill/mbart/wmt_en_tr
elif [[ $lang_pair == en-de ]]; then
    src=en_XX
    tgt=de_DE
    model_dir=$root/models/mbart-ende
    gold_dir=$root/distill/mbart/wmt_en_de
elif [[ $lang_pair == de-en ]]; then
    src=de_DE
    tgt=en_XX
    model_dir=$root/models/mbart-deen
    gold_dir=$root/distill/mbart/wmt_en_de
else
    echo "invalid lang_pair"
    exit
fi

bpe_model=$root/models/mbart.cc25.v2/sentence.bpe.model
DICT=$root/models/mbart.cc25.v2/dict.txt

spm_encode=$root/distill/scripts/spm_encode.py
spm_decode=$root/distill/scripts/spm_decode.py

REPLACE_UNICODE_PUNCT=$root/mosesdecoder/scripts/tokenizer/replace-unicode-punctuation.perl
NORM_PUNC=$root/mosesdecoder/scripts/tokenizer/normalize-punctuation.perl
REM_NON_PRINT_CHAR=$root/mosesdecoder/scripts/tokenizer/remove-non-printing-char.perl
REMOVE_DIACRITICS=$root/wmt16-scripts/preprocess/remove-diacritics.py
NORMALIZE_ROMANIAN=$root/wmt16-scripts/preprocess/normalise-romanian.py
TOKENIZER=$root/mosesdecoder/scripts/tokenizer/tokenizer.perl
DETOKENIZER=$root/mosesdecoder/scripts/tokenizer/detokenizer.perl

noise=tdrop$2
echo "noise=$noise"

seed=1
echo "seed=$seed"

retain_dropout=true
if [ $noise == tdrop0.0 ]; then
    retain_dropout=false
fi
echo "retain_dropout=$retain_dropout"

# read -n 1 -p "verify config (y/n) " ans
# echo ""
# if [[ $ans != y ]]; then
#     exit
# fi

data=$root/distill/mbart/data/$lang_pair
data_bin=$root/distill/mbart/data-bin/$lang_pair
DATA=$root/distill/mbart/data/$noise/$lang_pair
DATA_BIN=$root/distill/mbart/data-bin/$noise/$lang_pair

mkdir -p $data
mkdir -p $data_bin
mkdir -p $DATA
mkdir -p $DATA_BIN

### plain text to spm

# for split in train valid test; do
#     python $spm_encode --model=$bpe_model < $gold_dir/$split.$src > $data/$split.spm.$src
#     python $spm_encode --model=$bpe_model < $gold_dir/$split.$tgt > $data/$split.spm.$tgt
# done

### spm to binary

# fairseq-preprocess \
#     --source-lang $src \
#     --target-lang $tgt \
#     --srcdict $DICT \
#     --tgtdict $DICT \
#     --trainpref $data/train.spm \
#     --validpref $data/valid.spm \
#     --testpref $data/test.spm \
#     --destdir $data_bin \
#     --workers 70

### binary to plain text (generated by teacher with dropout)

if [ $noise == gold ]; then
    echo gold

    perl $DETOKENIZER -l ${src:0:2} < $gold_dir/train.$src > $DATA/train.$lang_pair.src
    perl $DETOKENIZER -l ${tgt:0:2} < $gold_dir/train.$tgt > $DATA/train.$lang_pair.sys
else

for split in train; do
    dest=$DATA/$split

    model_pt=model.pt
    if [[ $noise == tdrop0.1 ]]; then
        model_pt=model.tdrop0.1.pt
    fi
    if [[ $noise == tdrop0.2 ]]; then
        model_pt=model.tdrop0.2.pt
    fi

    cmd="fairseq-generate $data_bin --path $model_dir/$model_pt --task translation_from_pretrained_bart --gen-subset $split -s $src -t $tgt --batch-size 128 --langs $langs --seed $seed --bf16"
    if $retain_dropout; then
        cmd="$cmd --retain-dropout"
    fi
    echo $cmd

    # rm nohup.out
    echo "Writing output to $dest.$lang_pair.out"
    rm -f $dest.$lang_pair.out
    $cmd > $dest.$lang_pair.out
    echo "Prediction done"
    
    if [[ $split == test ]]; then
        cat $dest.$lang_pair.out | grep -P "^S" | sort -V | cut -f 2- | sed 's/\['$src'\]//g' > $dest.$lang_pair.src.temp
        cat $dest.$lang_pair.out | grep -P "^H" | sort -V | cut -f 3- | sed 's/\['$tgt'\]//g' > $dest.$lang_pair.sys.temp
    else
        cat $dest.$lang_pair.out | grep -P "^S" | cut -f 2- | sed 's/\['$src'\]//g' > $dest.$lang_pair.src.temp
        cat $dest.$lang_pair.out | grep -P "^H" | cut -f 3- | sed 's/\['$tgt'\]//g' > $dest.$lang_pair.sys.temp
    fi
    
    $spm_decode --model=$bpe_model --input=$dest.$lang_pair.src.temp > $dest.$lang_pair.src
    $spm_decode --model=$bpe_model --input=$dest.$lang_pair.sys.temp > $dest.$lang_pair.sys

    perl $DETOKENIZER -l ${src:0:2} < $dest.$lang_pair.src > $dest.$lang_pair.src.detok
    perl $DETOKENIZER -l ${tgt:0:2} < $dest.$lang_pair.sys > $dest.$lang_pair.sys.detok
    
    if [[ $split == test ]]; then
        cat $dest.$lang_pair.sys.detok | sacrebleu -t wmt17 -l $lang_pair
    fi

    rm $dest.$lang_pair.src $dest.$lang_pair.sys
    mv $dest.$lang_pair.src.detok $dest.$lang_pair.src
    mv $dest.$lang_pair.sys.detok $dest.$lang_pair.sys
    rm $DATA/*.temp
done

fi

### plain text to bpe

# # clean valid/test
# for split in valid test; do
#     perl $DETOKENIZER -l ${src:0:2} < $gold_dir/$split.$src > $gold_dir/$split.$src.clean
#     perl $DETOKENIZER -l ${tgt:0:2} < $gold_dir/$split.$tgt > $gold_dir/$split.$tgt.clean
# done

###

echo "encoding with $bpe_model"
python $spm_encode --model=$bpe_model < $DATA/train.$lang_pair.src > $DATA/train.spm.$lang_pair.$src
python $spm_encode --model=$bpe_model < $DATA/train.$lang_pair.sys > $DATA/train.spm.$lang_pair.$tgt

for split in valid test; do
    python $spm_encode --model=$bpe_model < $gold_dir/$split.$src.clean > $DATA/$split.spm.$lang_pair.$src
    python $spm_encode --model=$bpe_model < $gold_dir/$split.$tgt.clean > $DATA/$split.spm.$lang_pair.$tgt
done

### bpe to binary

echo "*.spm"
trainpref=train.spm.$lang_pair
validpref=valid.spm.$lang_pair
testpref=test.spm.$lang_pair
destdir=$DATA_BIN/spm
dictdir=$root/distill/mbart/data-bin/gold/$lang_pair/spm

echo "creating $destdir"
mkdir -p $destdir
rm $destdir/dict.* $destdir/preprocess.log

if [[ $noise == gold ]]; then
    echo "new dicts"
    fairseq-preprocess \
        --joined-dictionary \
        --source-lang $src \
        --target-lang $tgt \
        --trainpref $DATA/$trainpref \
        --validpref $DATA/$validpref \
        --testpref  $DATA/$testpref \
        --destdir $destdir \
        --workers 70
else
    echo "existing dicts"
    fairseq-preprocess \
        --srcdict $dictdir/dict.$src.txt \
        --tgtdict $dictdir/dict.$tgt.txt \
        --source-lang $src \
        --target-lang $tgt \
        --trainpref $DATA/$trainpref \
        --validpref $DATA/$validpref \
        --testpref  $DATA/$testpref \
        --destdir $destdir \
        --workers 70
fi
